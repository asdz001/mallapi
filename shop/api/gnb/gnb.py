import os
import sys
import logging
from pathlib import Path
from datetime import datetime, date
from collections import defaultdict
import re

# âœ… Django í™˜ê²½ ì„¤ì •
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
os.environ.setdefault("DJANGO_SETTINGS_MODULE", "mallapi.settings")

import django
django.setup()

import csv
import io
from ftplib import FTP
from django.db import transaction
from shop.models import RawProduct, RawProductOption

# âœ… ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('gnb_sync.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# âœ… ì„¤ì •
FTP_HOST = "93.46.41.5"
FTP_USER = "milanese"
FTP_PSW = "X90P6jYT3Gl!"
RETAILER_CODE = "IT-G-01"
EXPORT_DIR = Path("export") / "GNB"
PROCESSED_FILE_LIST = EXPORT_DIR / "processed_files.txt"
MAIN_FILES_PROCESSED = EXPORT_DIR / "main_files_processed.txt"  # ì²˜ë¦¬ëœ ì „ì²´íŒŒì¼ ëª©ë¡
EXPORT_DIR.mkdir(parents=True, exist_ok=True)
BATCH_SIZE = 1000

# âœ… íŒŒì¼ëª… íŒ¨í„´
MAIN_FILE_PATTERN = re.compile(r'COMPANY_\d+_\d+_(\d{4}-\d{2}-\d{2})_(\d{2}-\d{2}-\d{2})_0000001\.csv')
PARTIAL_FILE_PATTERN = re.compile(r'COMPANY_\d+_\d+_(\d{4}-\d{2}-\d{2})_(\d{2}-\d{2}-\d{2})_(\d{7})\.csv')

# âœ… í•„ë“œ ë§¤í•‘
COLUMNS = {
    "product_id": "IGUArticolo",
    "brand": "DSLinea",
    "name": "DSArticoloAgg",
    "model": "Modello",
    "gender": "DSSessoWeb",
    "category1": "DSRepartoWeb",
    "category2": "DSCategoriaMerceologicaWeb",
    "color": "Classificazione7",
    "description": "ArticoloDescrizionePers",
    "size": "Taglia",
    "stock": "Disponibilita",
    "price_org": "Costo",
    "price_retail": "PrezzoIvato",
    "season": "DSStagione",
    "origin": "DSMarca",
    "external_option_id": "IDArtCod",
    "material": "DSMateriale",
    "image_urls": ["URLImg1", "URLImg2", "URLImg3", "URLImg4"]
}

# âœ… í†µê³„ í´ë˜ìŠ¤
class ProcessingStats:
    def __init__(self):
        self.processed_files = 0
        self.main_files_processed = 0
        self.partial_files_processed = 0
        self.total_products = 0
        self.total_options = 0
        self.filtered_zero_stock = 0
        self.new_products = 0
        self.updated_products = 0
        self.deleted_products = 0
        self.errors = []
        self.start_time = datetime.now()
    
    def add_error(self, error_msg):
        self.errors.append(error_msg)
        logger.error(error_msg)
    
    def get_summary(self):
        duration = datetime.now() - self.start_time
        return {
            'processed_files': self.processed_files,
            'main_files_processed': self.main_files_processed,
            'partial_files_processed': self.partial_files_processed,
            'total_products': self.total_products,
            'total_options': self.total_options,
            'filtered_zero_stock': self.filtered_zero_stock,
            'new_products': self.new_products,
            'updated_products': self.updated_products,
            'deleted_products': self.deleted_products,
            'duration': str(duration),
            'error_count': len(self.errors)
        }

# âœ… íŒŒì¼ ì •ë³´ í´ë˜ìŠ¤
class FileInfo:
    def __init__(self, filename):
        self.filename = filename
        self.is_main = filename.endswith("_0000001.csv")
        self.date = self.extract_date()
        self.time = self.extract_time()
        self.sequence = self.extract_sequence()
        self.sort_key = f"{self.date}_{self.time}_{self.sequence:07d}"
    
    def extract_date(self):
        """íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ"""
        pattern = MAIN_FILE_PATTERN if self.is_main else PARTIAL_FILE_PATTERN
        match = pattern.search(self.filename)
        if match:
            return datetime.strptime(match.group(1), '%Y-%m-%d').date()
        return None
    
    def extract_time(self):
        """íŒŒì¼ëª…ì—ì„œ ì‹œê°„ ì¶”ì¶œ"""
        pattern = MAIN_FILE_PATTERN if self.is_main else PARTIAL_FILE_PATTERN
        match = pattern.search(self.filename)
        if match:
            return match.group(2)  # HH-MM-SS í˜•íƒœ
        return "00-00-00"
    
    def extract_sequence(self):
        """íŒŒì¼ëª…ì—ì„œ ì‹œí€€ìŠ¤ ë²ˆí˜¸ ì¶”ì¶œ"""
        if self.is_main:
            return 1  # ì „ì²´íŒŒì¼ì€ í•­ìƒ 1
        else:
            match = PARTIAL_FILE_PATTERN.search(self.filename)
            if match:
                return int(match.group(3))
        return 0

# âœ… ì•ˆì „í•œ ë³€í™˜ í•¨ìˆ˜ë“¤
def safe_float_convert(value, default=0.0):
    try:
        if not value or value.strip() == '':
            return default
        return float(value.replace(',', ''))
    except (ValueError, TypeError, AttributeError):
        logger.warning(f"Float ë³€í™˜ ì‹¤íŒ¨: {value}")
        return default

def safe_int_convert(value, default=0):
    try:
        if not value or value.strip() == '':
            return default
        return int(float(str(value).replace(',', '')))
    except (ValueError, TypeError, AttributeError):
        logger.warning(f"Int ë³€í™˜ ì‹¤íŒ¨: {value}")
        return default

def safe_string_convert(value, default=""):
    try:
        return str(value).strip() if value else default
    except:
        return default

# âœ… íŒŒì¼ ê´€ë¦¬ í•¨ìˆ˜ë“¤
def get_processed_files():
    """ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
    try:
        return set(PROCESSED_FILE_LIST.read_text().splitlines()) if PROCESSED_FILE_LIST.exists() else set()
    except Exception as e:
        logger.error(f"ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return set()

def get_processed_main_files():
    """ì²˜ë¦¬ëœ ì „ì²´íŒŒì¼ ëª©ë¡ ì¡°íšŒ (ë‚ ì§œë³„)"""
    try:
        processed_mains = {}
        if MAIN_FILES_PROCESSED.exists():
            for line in MAIN_FILES_PROCESSED.read_text().splitlines():
                if line.strip():
                    parts = line.split('\t')
                    if len(parts) >= 2:
                        date_str = parts[0]
                        filename = parts[1]
                        file_date = datetime.strptime(date_str, '%Y-%m-%d').date()
                        processed_mains[file_date] = filename
        return processed_mains
    except Exception as e:
        logger.error(f"ì²˜ë¦¬ëœ ì „ì²´íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {}

def mark_processed(filename):
    """íŒŒì¼ì„ ì²˜ë¦¬ ì™„ë£Œ ëª©ë¡ì— ì¶”ê°€"""
    try:
        with PROCESSED_FILE_LIST.open("a", encoding='utf-8') as f:
            f.write(f"{filename}\n")
    except Exception as e:
        logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ í‘œì‹œ ì‹¤íŒ¨: {e}")

def mark_main_processed(filename, file_date):
    """ì „ì²´íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ í‘œì‹œ"""
    try:
        with MAIN_FILES_PROCESSED.open("a", encoding='utf-8') as f:
            f.write(f"{file_date.strftime('%Y-%m-%d')}\t{filename}\t{datetime.now().isoformat()}\n")
    except Exception as e:
        logger.error(f"ì „ì²´íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ í‘œì‹œ ì‹¤íŒ¨: {e}")

def get_last_processed_file_for_date(target_date, all_files):
    """íŠ¹ì • ë‚ ì§œì˜ ë§ˆì§€ë§‰ ì²˜ë¦¬ëœ íŒŒì¼ ì°¾ê¸°"""
    processed_files = get_processed_files()
    
    # í•´ë‹¹ ë‚ ì§œì˜ íŒŒì¼ë“¤ ì¤‘ ì²˜ë¦¬ëœ ê²ƒë“¤ë§Œ í•„í„°ë§
    date_files = [f for f in all_files if f.date == target_date and f.filename in processed_files]
    
    if not date_files:
        return None
    
    # ì‹œê°„ìˆœ, ì‹œí€€ìŠ¤ìˆœ ì •ë ¬í•´ì„œ ë§ˆì§€ë§‰ íŒŒì¼ ë°˜í™˜
    date_files.sort(key=lambda x: x.sort_key)
    return date_files[-1]

# âœ… ë°ì´í„° ê²€ì¦
def validate_product_data(product_data):
    """ìƒí’ˆ ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬"""
    if not product_data.get('external_product_id'):
        return False, "ìƒí’ˆ ID ëˆ„ë½"
    
    if not product_data.get('product_name', '').strip():
        return False, "ìƒí’ˆëª… ëˆ„ë½"
    
    if product_data.get('price_retail', 0) < 0:
        return False, "íŒë§¤ê°€ê²© ì˜¤ë¥˜"
    
    return True, None

# âœ… CSV íŒŒì„œ
def parse_csv(data, stats):
    """CSV ë°ì´í„° íŒŒì‹±"""
    try:
        content = data.decode("utf-8")
        reader = csv.DictReader(io.StringIO(content))
        
        products_dict = defaultdict(lambda: {
            'product_info': None,
            'options': []
        })
        
        filtered_count = 0
        
        for row_num, row in enumerate(reader, start=2):
            try:
                if not row.get(COLUMNS["product_id"]):
                    continue
                
                # ì¬ê³  í™•ì¸ - 0ì´ë©´ ê±´ë„ˆë›°ê¸°
                stock = safe_int_convert(row.get(COLUMNS["stock"], 0))
                if stock <= 0:
                    filtered_count += 1
                    continue
                
                product_id = safe_string_convert(row[COLUMNS["product_id"]])
                
                # ìƒí’ˆ ê¸°ë³¸ ì •ë³´
                if products_dict[product_id]['product_info'] is None:
                    products_dict[product_id]['product_info'] = {
                        "external_product_id": product_id,
                        "product_name": f"{safe_string_convert(row.get(COLUMNS['brand'], ''))} {safe_string_convert(row.get(COLUMNS['name'], ''))} {safe_string_convert(row.get(COLUMNS['model'], ''))}".strip(),
                        "raw_brand_name": safe_string_convert(row.get(COLUMNS["brand"], "")),
                        "gender": safe_string_convert(row.get(COLUMNS["gender"], "")),
                        "category1": safe_string_convert(row.get(COLUMNS["category1"], "")),
                        "category2": safe_string_convert(row.get(COLUMNS["category2"], "")),
                        "color": safe_string_convert(row.get(COLUMNS["color"], "")),
                        "description": safe_string_convert(row.get(COLUMNS["description"], "")),
                        "price_org": safe_float_convert(row.get(COLUMNS["price_org"], 0)),
                        "price_retail": safe_float_convert(row.get(COLUMNS["price_retail"], 0)),
                        "sku": safe_string_convert(row.get(COLUMNS["model"], "")),
                        "season": safe_string_convert(row.get(COLUMNS["season"], "")),
                        "material": safe_string_convert(row.get(COLUMNS["material"], "")),
                        "origin": safe_string_convert(row.get(COLUMNS["origin"], "")),
                        "image_url_1": safe_string_convert(row.get(COLUMNS["image_urls"][0], "")),
                        "image_url_2": safe_string_convert(row.get(COLUMNS["image_urls"][1], "")),
                        "image_url_3": safe_string_convert(row.get(COLUMNS["image_urls"][2], "")),
                        "image_url_4": safe_string_convert(row.get(COLUMNS["image_urls"][3], "")),
                    }
                
                # ì˜µì…˜ ì •ë³´ ì¶”ê°€
                option = {
                    "option_name": safe_string_convert(row.get(COLUMNS["size"], "ONE")),
                    "stock": stock,
                    "price": safe_float_convert(row.get(COLUMNS["price_org"], 0)),
                    "external_option_id": safe_string_convert(row.get(COLUMNS["external_option_id"], "")),
                }
                products_dict[product_id]['options'].append(option)
                
            except Exception as e:
                stats.add_error(f"í–‰ {row_num} íŒŒì‹± ì˜¤ë¥˜: {e}")
                continue
        
        # ìµœì¢… ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        final_products = []
        for product_id, data in products_dict.items():
            if data['product_info'] and data['options']:
                is_valid, error_msg = validate_product_data(data['product_info'])
                if not is_valid:
                    stats.add_error(f"ìƒí’ˆ {product_id}: {error_msg}")
                    continue
                
                product_with_options = data['product_info'].copy()
                product_with_options['options'] = data['options']
                final_products.append(product_with_options)
        
        stats.filtered_zero_stock = filtered_count
        stats.total_options = sum(len(p['options']) for p in final_products)
        
        logger.info(f"CSV íŒŒì‹± ì™„ë£Œ: ìƒí’ˆ {len(final_products)}ê°œ, ì˜µì…˜ {stats.total_options}ê°œ (ì¬ê³ 0 í•„í„°ë§: {filtered_count}ê°œ)")
        return final_products
        
    except Exception as e:
        stats.add_error(f"CSV íŒŒì‹± ì‹¤íŒ¨: {e}")
        return []

# âœ… DB ë“±ë¡ í•¨ìˆ˜ë“¤
def register_full_sync(products, stats):
    """ì „ì²´ ë™ê¸°í™” (ì „ì²´íŒŒì¼ ê¸°ì¤€)"""
    try:
        logger.info("ì „ì²´ ë™ê¸°í™” ì‹œì‘ - ì „ì²´íŒŒì¼ ê¸°ì¤€ìœ¼ë¡œ ì™„ì „ ë™ê¸°í™”")
        
        # 1. ê¸°ì¡´ ë°ì´í„° ì¡°íšŒ
        existing_products = RawProduct.objects.filter(retailer=RETAILER_CODE)
        existing_map = {p.external_product_id: p for p in existing_products}
        existing_ids = set(existing_map.keys())
        
        # 2. ë“¤ì–´ì˜¤ëŠ” ìƒí’ˆ ID ì„¸íŠ¸
        incoming_ids = set(p["external_product_id"] for p in products)

        RawProduct.objects.filter(
            external_product_id__in=incoming_ids,
            retailer=RETAILER_CODE,
            status="soldout"
        ).update(status="pending")
        
        # 3. ì‹ ê·œ/ì—…ë°ì´íŠ¸/ì‚­ì œ ë¶„ë¥˜
        new_products = []
        update_products = []
        all_options = []
        to_delete_ids = existing_ids - incoming_ids  # âœ… ì „ì²´íŒŒì¼ì— ì—†ëŠ” ìƒí’ˆë“¤ë§Œ ì‚­ì œ
        
        for item in products:
            pid = item["external_product_id"]
            options_data = item.pop("options")
            
            if pid in existing_map:
                # ê¸°ì¡´ ìƒí’ˆ ì—…ë°ì´íŠ¸
                product = existing_map[pid]
                for key, value in item.items():
                    setattr(product, key, value)
                update_products.append(product)
                stats.updated_products += 1
            else:
                # ì‹ ê·œ ìƒí’ˆ
                product = RawProduct(retailer=RETAILER_CODE, **item)
                new_products.append(product)
                stats.new_products += 1
            
            # ì˜µì…˜ ì €ì¥
            for option_data in options_data:
                all_options.append((pid, option_data))
        
        # 4. DB ì‘ì—… (íŠ¸ëœì­ì…˜)
        with transaction.atomic():
            # ì‹ ê·œ ìƒí’ˆ ë“±ë¡
            if new_products:
                RawProduct.objects.bulk_create(new_products, batch_size=BATCH_SIZE)
                logger.info(f"ì‹ ê·œ ìƒí’ˆ ë“±ë¡: {len(new_products)}ê°œ")
            
            # ê¸°ì¡´ ìƒí’ˆ ì—…ë°ì´íŠ¸
            if update_products:
                RawProduct.objects.bulk_update(
                    update_products, 
                    fields=[k for k in item.keys()], 
                    batch_size=BATCH_SIZE
                )
                logger.info(f"ê¸°ì¡´ ìƒí’ˆ ì—…ë°ì´íŠ¸: {len(update_products)}ê°œ")
            
            # âœ… ì „ì²´íŒŒì¼ì— ì—†ëŠ” ìƒí’ˆ ì‚­ì œ (ì „ì²´íŒŒì¼ì¼ ë•Œë§Œ)
            if to_delete_ids:
                updated_count = RawProduct.objects.filter(
                    external_product_id__in=to_delete_ids,
                    retailer=RETAILER_CODE
                ).update(status="soldout")

                logger.info(f"ì „ì²´íŒŒì¼ ê¸°ì¤€ soldout ì²˜ë¦¬ëœ ìƒí’ˆ: {updated_count}ê°œ")
            
            # ëª¨ë“  ì˜µì…˜ ì¬ë“±ë¡
            RawProductOption.objects.filter(
                product__retailer=RETAILER_CODE
            ).delete()
            
            # ìƒˆ ì˜µì…˜ ë“±ë¡
            product_map = {p.external_product_id: p for p in 
                          RawProduct.objects.filter(external_product_id__in=incoming_ids, retailer=RETAILER_CODE)}
            
            options_to_create = []
            for pid, option_data in all_options:
                if pid in product_map:
                    options_to_create.append(RawProductOption(
                        product=product_map[pid],
                        option_name=option_data["option_name"],
                        external_option_id=option_data["external_option_id"],
                        stock=option_data["stock"],
                        price=option_data["price"]
                    ))
            
            if options_to_create:
                RawProductOption.objects.bulk_create(options_to_create, batch_size=BATCH_SIZE)
                logger.info(f"ì˜µì…˜ ë“±ë¡: {len(options_to_create)}ê°œ")
        
        stats.total_products = len(products)
        logger.info("ì „ì²´ ë™ê¸°í™” ì™„ë£Œ")
        
    except Exception as e:
        stats.add_error(f"ì „ì²´ ë™ê¸°í™” ì‹¤íŒ¨: {e}")
        raise

def register_partial_update(products, stats):
    """ë¶€ë¶„ ì—…ë°ì´íŠ¸ (ë¶€ë¶„íŒŒì¼ ê¸°ì¤€) - ì‚­ì œ ì—†ìŒ"""
    try:
        logger.info("ë¶€ë¶„ ì—…ë°ì´íŠ¸ ì‹œì‘ - ìƒí’ˆ ì‚­ì œ ì—†ì´ ì¶”ê°€/ìˆ˜ì •ë§Œ")
        
        # 1. ê¸°ì¡´ ë°ì´í„° ì¡°íšŒ
        updating_product_ids = [p["external_product_id"] for p in products]
        existing_products = RawProduct.objects.filter(
            external_product_id__in=updating_product_ids,
            retailer=RETAILER_CODE
        )
        existing_map = {p.external_product_id: p for p in existing_products}
        
        # 2. ì‹ ê·œ/ì—…ë°ì´íŠ¸ ë¶„ë¥˜
        new_products = []
        update_products = []
        all_options = []
        
        for item in products:
            pid = item["external_product_id"]
            options_data = item.pop("options")
            
            if pid in existing_map:
                # ê¸°ì¡´ ìƒí’ˆ ì—…ë°ì´íŠ¸
                product = existing_map[pid]
                for key, value in item.items():
                    setattr(product, key, value)
                update_products.append(product)
                stats.updated_products += 1
            else:
                # ì‹ ê·œ ìƒí’ˆ
                product = RawProduct(retailer=RETAILER_CODE, **item)
                new_products.append(product)
                stats.new_products += 1
            
            # ì˜µì…˜ ì €ì¥
            for option_data in options_data:
                all_options.append((pid, option_data))
        
        # 3. DB ì‘ì—… (íŠ¸ëœì­ì…˜)
        with transaction.atomic():
            # ì‹ ê·œ ìƒí’ˆ ë“±ë¡
            if new_products:
                RawProduct.objects.bulk_create(new_products, batch_size=BATCH_SIZE)
                logger.info(f"ì‹ ê·œ ìƒí’ˆ ë“±ë¡: {len(new_products)}ê°œ")
            
            # ê¸°ì¡´ ìƒí’ˆ ì—…ë°ì´íŠ¸
            if update_products:
                RawProduct.objects.bulk_update(
                    update_products, 
                    fields=[k for k in item.keys()], 
                    batch_size=BATCH_SIZE
                )
                logger.info(f"ê¸°ì¡´ ìƒí’ˆ ì—…ë°ì´íŠ¸: {len(update_products)}ê°œ")
            
            # âœ… ì—…ë°ì´íŠ¸ë˜ëŠ” ìƒí’ˆë“¤ì˜ ê¸°ì¡´ ì˜µì…˜ë§Œ ì‚­ì œ (í•´ë‹¹ ìƒí’ˆì˜ ì˜µì…˜ë§Œ)
            RawProductOption.objects.filter(
                product__external_product_id__in=updating_product_ids,
                product__retailer=RETAILER_CODE
            ).delete()
            
            # ìƒˆ ì˜µì…˜ ë“±ë¡
            product_map = {p.external_product_id: p for p in 
                          RawProduct.objects.filter(external_product_id__in=updating_product_ids, retailer=RETAILER_CODE)}
            
            options_to_create = []
            for pid, option_data in all_options:
                if pid in product_map:
                    options_to_create.append(RawProductOption(
                        product=product_map[pid],
                        option_name=option_data["option_name"],
                        external_option_id=option_data["external_option_id"],
                        stock=option_data["stock"],
                        price=option_data["price"]
                    ))
            
            if options_to_create:
                RawProductOption.objects.bulk_create(options_to_create, batch_size=BATCH_SIZE)
                logger.info(f"ì˜µì…˜ ë“±ë¡: {len(options_to_create)}ê°œ")
        
        stats.total_products += len(products)
        logger.info("ë¶€ë¶„ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        
    except Exception as e:
        stats.add_error(f"ë¶€ë¶„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        raise

# âœ… FTP ë° íŒŒì¼ ì²˜ë¦¬
def connect_ftp():
    """FTP ì—°ê²°"""
    try:
        ftp = FTP(FTP_HOST)
        ftp.login(user=FTP_USER, passwd=FTP_PSW)
        logger.info("FTP ì—°ê²° ì„±ê³µ")
        return ftp
    except Exception as e:
        logger.error(f"FTP ì—°ê²° ì‹¤íŒ¨: {e}")
        raise

def get_files_to_process(ftp):
    """âœ… ì˜¤ëŠ˜ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ì²˜ë¦¬í•  íŒŒì¼ ëª©ë¡ ê²°ì •"""
    try:
        today = date.today()
        
        # 1. ëª¨ë“  CSV íŒŒì¼ ì¡°íšŒ ë° ë¶„ì„
        all_files = [f for f in ftp.nlst() if f.endswith(".csv")]
        file_infos = []
        
        for filename in all_files:
            file_info = FileInfo(filename)
            if file_info.date:  # ë‚ ì§œ íŒŒì‹± ì„±ê³µí•œ íŒŒì¼ë§Œ
                file_infos.append(file_info)
        
        # 2. ì˜¤ëŠ˜ ë‚ ì§œ íŒŒì¼ë“¤ë§Œ í•„í„°ë§
        today_files = [f for f in file_infos if f.date == today]
        
        if not today_files:
            logger.info("ì˜¤ëŠ˜ ë‚ ì§œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None, []
        
        # 3. ì‹œê°„ìˆœ, ì‹œí€€ìŠ¤ìˆœ ì •ë ¬
        today_files.sort(key=lambda x: x.sort_key)
        
        # 4. ì „ì²´íŒŒì¼ê³¼ ë¶€ë¶„íŒŒì¼ ë¶„ë¦¬
        main_file = None
        partial_files = []
        
        for file_info in today_files:
            if file_info.is_main:
                main_file = file_info
            else:
                partial_files.append(file_info)
        
        # 5. ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ë“¤ í™•ì¸
        processed_files = get_processed_files()
        processed_main_files = get_processed_main_files()
        
        # 6. ì „ì²´íŒŒì¼ ì²˜ë¦¬ ì—¬ë¶€ í™•ì¸
        main_to_process = None
        if main_file and main_file.filename not in processed_files:
            main_to_process = main_file
            logger.info(f"ì˜¤ëŠ˜ ë‚ ì§œ ì „ì²´íŒŒì¼ ë°œê²¬: {main_file.filename}")
        elif today in processed_main_files:
            logger.info(f"ì˜¤ëŠ˜ ë‚ ì§œ ì „ì²´íŒŒì¼ ì´ë¯¸ ì²˜ë¦¬ë¨: {processed_main_files[today]}")
        else:
            logger.info("ì˜¤ëŠ˜ ë‚ ì§œ ì „ì²´íŒŒì¼ ì—†ìŒ")
        
        # 7. ë¶€ë¶„íŒŒì¼ ì²˜ë¦¬ ëŒ€ìƒ ê²°ì •
        partials_to_process = []
        
        if main_to_process:
            # ì „ì²´íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” ê²½ìš°: ì „ì²´íŒŒì¼ë³´ë‹¤ ëŠ¦ì€ ë¶€ë¶„íŒŒì¼ë“¤ë§Œ
            partials_to_process = [f for f in partial_files 
                                 if f.sort_key > main_to_process.sort_key 
                                 and f.filename not in processed_files]
        else:
            # ì „ì²´íŒŒì¼ì´ ì´ë¯¸ ì²˜ë¦¬ëœ ê²½ìš°: ë§ˆì§€ë§‰ ì²˜ë¦¬ëœ íŒŒì¼ ì´í›„ì˜ ë¶€ë¶„íŒŒì¼ë“¤ë§Œ
            last_processed = get_last_processed_file_for_date(today, today_files)
            
            if last_processed:
                partials_to_process = [f for f in partial_files 
                                     if f.sort_key > last_processed.sort_key 
                                     and f.filename not in processed_files]
            else:
                # ì•„ë¬´ê²ƒë„ ì²˜ë¦¬ ì•ˆí–ˆìœ¼ë©´ ëª¨ë“  ë¶€ë¶„íŒŒì¼
                partials_to_process = [f for f in partial_files 
                                     if f.filename not in processed_files]
        
        logger.info(f"ì²˜ë¦¬ ëŒ€ìƒ: ì „ì²´íŒŒì¼ {1 if main_to_process else 0}ê°œ, ë¶€ë¶„íŒŒì¼ {len(partials_to_process)}ê°œ")
        
        return main_to_process, partials_to_process
        
    except Exception as e:
        logger.error(f"íŒŒì¼ ëª©ë¡ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return None, []

def download_and_process_file(ftp, file_info, stats):
    """íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° ì²˜ë¦¬"""
    try:
        filename = file_info.filename
        
        # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        buffer = io.BytesIO()
        ftp.retrbinary(f"RETR {filename}", buffer.write)
        buffer.seek(0)
        
        # CSV íŒŒì‹±
        products = parse_csv(buffer.read(), stats)
        if not products:
            logger.warning(f"íŒŒì¼ì—ì„œ ìœ íš¨í•œ ìƒí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {filename}")
            return
        
        # íŒŒì¼ íƒ€ì…ì— ë”°ë¼ ì²˜ë¦¬
        if file_info.is_main:
            logger.info(f"ğŸ“¥ ì „ì²´íŒŒì¼ ì²˜ë¦¬: {filename} ({len(products)}ê°œ ìƒí’ˆ)")
            register_full_sync(products, stats)
            mark_main_processed(filename, file_info.date)
            stats.main_files_processed += 1
        else:
            logger.info(f"ğŸ“¥ ë¶€ë¶„íŒŒì¼ ì²˜ë¦¬: {filename} ({len(products)}ê°œ ìƒí’ˆ)")
            register_partial_update(products, stats)
            stats.partial_files_processed += 1
        
        # ì²˜ë¦¬ ì™„ë£Œ í‘œì‹œ
        mark_processed(filename)
        stats.processed_files += 1
        
    except Exception as e:
        stats.add_error(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ ({filename}): {e}")

# âœ… ë©”ì¸ ì‹¤í–‰ ë¡œì§
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    stats = ProcessingStats()
    ftp = None
    
    try:
        logger.info("=== GNB ìƒí’ˆ ë™ê¸°í™” ì‹œì‘ ===")
        logger.info(f"ì‹¤í–‰ ë‚ ì§œ: {date.today()}")
        
        # FTP ì—°ê²°
        ftp = connect_ftp()
        
        # âœ… ì˜¤ëŠ˜ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ì²˜ë¦¬í•  íŒŒì¼ ê²°ì •
        main_file, partial_files = get_files_to_process(ftp)
        
        if not main_file and not partial_files:
            logger.info("ì˜¤ëŠ˜ ë‚ ì§œì— ì²˜ë¦¬í•  ìƒˆ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # âœ… 1ë‹¨ê³„: ì „ì²´íŒŒì¼ ì²˜ë¦¬ (ìˆëŠ” ê²½ìš°)
        if main_file:
            try:
                logger.info(f"=== ì „ì²´íŒŒì¼ ì²˜ë¦¬ ì‹œì‘ ===")
                download_and_process_file(ftp, main_file, stats)
                logger.info(f"=== ì „ì²´íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ ===")
            except Exception as e:
                stats.add_error(f"ì „ì²´íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ({main_file.filename}): {e}")
                logger.error("ì „ì²´íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ë¡œ ì¸í•´ ë¶€ë¶„íŒŒì¼ ì²˜ë¦¬ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                return  # ì „ì²´íŒŒì¼ ì‹¤íŒ¨ì‹œ ë¶€ë¶„íŒŒì¼ ì²˜ë¦¬ ì•ˆí•¨
        
        # âœ… 2ë‹¨ê³„: ë¶€ë¶„íŒŒì¼ ì²˜ë¦¬ (ìˆœì„œëŒ€ë¡œ)
        if partial_files:
            logger.info(f"=== ë¶€ë¶„íŒŒì¼ ì²˜ë¦¬ ì‹œì‘ ({len(partial_files)}ê°œ) ===")
            for file_info in partial_files:
                try:
                    download_and_process_file(ftp, file_info, stats)
                except Exception as e:
                    # ê°œë³„ ë¶€ë¶„íŒŒì¼ ì‹¤íŒ¨ëŠ” ë‹¤ìŒ íŒŒì¼ ê³„ì† ì²˜ë¦¬
                    stats.add_error(f"ë¶€ë¶„íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ({file_info.filename}): {e}")
                    continue
            logger.info(f"=== ë¶€ë¶„íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ ===")
        
    except Exception as e:
        stats.add_error(f"ì „ì²´ ì²˜ë¦¬ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        
    finally:
        # FTP ì—°ê²° ì¢…ë£Œ
        if ftp:
            try:
                ftp.quit()
            except:
                pass
        
        # âœ… ìƒì„¸ ê²°ê³¼ ë¦¬í¬íŠ¸
        summary = stats.get_summary()
        logger.info("=== ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ===")
        logger.info(f"ì‹¤í–‰ ë‚ ì§œ: {date.today()}")
        logger.info(f"ì „ì²´ ì²˜ë¦¬ íŒŒì¼: {summary['processed_files']}ê°œ")
        logger.info(f"  â””â”€ ì „ì²´íŒŒì¼: {summary['main_files_processed']}ê°œ")
        logger.info(f"  â””â”€ ë¶€ë¶„íŒŒì¼: {summary['partial_files_processed']}ê°œ")
        logger.info(f"ì´ ìƒí’ˆ: {summary['total_products']}ê°œ")
        logger.info(f"ì´ ì˜µì…˜: {summary['total_options']}ê°œ")
        logger.info(f"ì¬ê³ 0 í•„í„°ë§: {summary['filtered_zero_stock']}ê°œ")
        logger.info(f"ì‹ ê·œ ìƒí’ˆ: {summary['new_products']}ê°œ")
        logger.info(f"ì—…ë°ì´íŠ¸ ìƒí’ˆ: {summary['updated_products']}ê°œ")
        logger.info(f"ì‚­ì œëœ ìƒí’ˆ: {summary['deleted_products']}ê°œ")
        logger.info(f"ì†Œìš” ì‹œê°„: {summary['duration']}")
        logger.info(f"ì˜¤ë¥˜ ê±´ìˆ˜: {summary['error_count']}ê±´")
        
        if stats.errors:
            logger.error("=== ì˜¤ë¥˜ ë‚´ì—­ ===")
            for error in stats.errors:
                logger.error(f"  - {error}")
        
        logger.info("=== GNB ìƒí’ˆ ë™ê¸°í™” ì™„ë£Œ ===")
        return stats.total_products  # âœ… í•„ìˆ˜

if __name__ == "__main__":
    main()