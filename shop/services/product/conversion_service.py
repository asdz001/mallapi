from shop.models import RawProduct, Product, RawProductOption, ProductOption
from django.db import transaction
from django.utils.timezone import now
from django.db.models import Sum, Prefetch
from dictionary.models import BrandAlias, CategoryLevel1Alias, CategoryLevel2Alias, CategoryLevel3Alias
from pricing.models import FixedCountry, CountryAlias
from eventlog.services.log_service import log_conversion_failure
from typing import Dict, List, Optional, Tuple
import logging
import time

logger = logging.getLogger(__name__)

class OptimizedConversionService:
    """ìµœì í™”ëœ ë³€í™˜ ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self):
        # ğŸš€ ìºì‹œ ì´ˆê¸°í™” - ê°€ì¥ í° ì„±ëŠ¥ ê°œì„  í¬ì¸íŠ¸
        self.brand_cache = {}
        self.category1_cache = {}
        self.category2_cache = {}
        self.category3_cache = {}
        self.country_cache = {}
        
        # í†µê³„
        self.stats = {
            'total_processed': 0,
            'success_count': 0,
            'fail_count': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'start_time': None
        }
        
        self._load_all_caches()
    
    def _load_all_caches(self):
        """ëª¨ë“  ë§¤í•‘ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ìºì‹œ"""
        start_time = time.time()
        logger.info("ğŸ”„ ë§¤í•‘ ë°ì´í„° ìºì‹œ ë¡œë”© ì¤‘...")
        
        # ğŸš€ ë¸Œëœë“œ ìºì‹œ - select_relatedë¡œ ì¿¼ë¦¬ ìµœì í™”
        brand_aliases = BrandAlias.objects.select_related('brand').all()
        for alias_obj in brand_aliases:
            aliases = [alias.strip().upper() for alias in alias_obj.alias.split(",")]
            for alias in aliases:
                self.brand_cache[alias] = alias_obj.brand.name
        
        # ğŸš€ ì¹´í…Œê³ ë¦¬ ìºì‹œë“¤
        cat1_aliases = CategoryLevel1Alias.objects.select_related('category').all()
        for alias_obj in cat1_aliases:
            aliases = [alias.strip().upper() for alias in alias_obj.alias.split(",")]
            for alias in aliases:
                self.category1_cache[alias] = alias_obj.category.name
        
        cat2_aliases = CategoryLevel2Alias.objects.select_related('category').all()
        for alias_obj in cat2_aliases:
            aliases = [alias.strip().upper() for alias in alias_obj.alias.split(",")]
            for alias in aliases:
                self.category2_cache[alias] = alias_obj.category.name
        
        cat3_aliases = CategoryLevel3Alias.objects.select_related('category').all()
        for alias_obj in cat3_aliases:
            aliases = [alias.strip().upper() for alias in alias_obj.alias.split(",")]
            for alias in aliases:
                self.category3_cache[alias] = alias_obj.category.name
        
        # ğŸš€ êµ­ê°€ ìºì‹œ
        country_aliases = CountryAlias.objects.select_related('standard_country').all()
        for alias_obj in country_aliases:
            aliases = [alias.strip().upper() for alias in alias_obj.origin_name.split(",")]
            for alias in aliases:
                self.country_cache[alias] = alias_obj.standard_country.name
        
        elapsed = time.time() - start_time
        logger.info(f"âœ… ìºì‹œ ë¡œë”© ì™„ë£Œ ({elapsed:.2f}ì´ˆ)")
        logger.info(f"   ë¸Œëœë“œ: {len(self.brand_cache)}ê°œ")
        logger.info(f"   ì¹´í…Œê³ ë¦¬1: {len(self.category1_cache)}ê°œ")
        logger.info(f"   ì¹´í…Œê³ ë¦¬2: {len(self.category2_cache)}ê°œ")
        logger.info(f"   ì¹´í…Œê³ ë¦¬3: {len(self.category3_cache)}ê°œ")
        logger.info(f"   êµ­ê°€: {len(self.country_cache)}ê°œ")
    
    def match_brand_cached(self, input_value: str) -> Optional[str]:
        """ìºì‹œëœ ë¸Œëœë“œ ë§¤ì¹­"""
        if not input_value:
            return None
        
        value = input_value.strip().upper()
        result = self.brand_cache.get(value)
        
        if result:
            self.stats['cache_hits'] += 1
        else:
            self.stats['cache_misses'] += 1
        
        return result
    
    def match_category_cached(self, cache_dict: Dict, input_value: str) -> Optional[str]:
        """ìºì‹œëœ ì¹´í…Œê³ ë¦¬ ë§¤ì¹­"""
        if not input_value:
            return None
        
        value = input_value.strip().upper()
        result = cache_dict.get(value)
        
        if result:
            self.stats['cache_hits'] += 1
        else:
            self.stats['cache_misses'] += 1
        
        return result
    
    def match_country_cached(self, input_value: str) -> Optional[str]:
        """ìºì‹œëœ êµ­ê°€ ë§¤ì¹­"""
        if not input_value:
            return None
        
        value = input_value.strip().upper()
        result = self.country_cache.get(value)
        
        if result:
            self.stats['cache_hits'] += 1
        else:
            self.stats['cache_misses'] += 1
        
        return result
    
    def validate_raw_product(self, raw_product: RawProduct) -> Tuple[bool, str]:
        """ìƒí’ˆ ê²€ì¦ ë¡œì§"""
        # ğŸš€ ì¬ê³  ì²´í¬ ìµœì í™” - ê°œë³„ ì˜µì…˜ ì²´í¬ê°€ ì•„ë‹Œ ì§‘ê³„ ì¿¼ë¦¬
        total_stock = raw_product.rawoptions.aggregate(total=Sum("stock"))['total'] or 0
        if total_stock <= 0:
            return False, "ì¬ê³  ì—†ìŒ"
        
        # ì›ê°€ ì²´í¬
        if not raw_product.price_org or raw_product.price_org == 0:
            return False, "ì›ê°€ ì—†ìŒ ë˜ëŠ” 0ì›"
        
        return True, ""
    
    def convert_single_product(self, raw_product: RawProduct) -> bool:
        """ë‹¨ì¼ ìƒí’ˆ ë³€í™˜ (ìµœì í™”)"""
        try:
            # 1. ê²€ì¦
            is_valid, error_reason = self.validate_raw_product(raw_product)
            if not is_valid:
                log_conversion_failure(raw_product, error_reason)
                logger.debug(f"âŒ [ê²€ì¦ì‹¤íŒ¨] {raw_product.external_product_id}: {error_reason}")
                return False
            
            # 2. ìºì‹œëœ ë§¤í•‘ ìˆ˜í–‰
            std_brand = self.match_brand_cached(raw_product.raw_brand_name)
            std_cat1 = self.match_category_cached(self.category1_cache, raw_product.gender)
            std_cat2 = self.match_category_cached(self.category2_cache, raw_product.category1)
            std_cat3 = self.match_category_cached(self.category3_cache, raw_product.category2)
            
            origin_input = (raw_product.origin or "").strip()
            origin_for_save = origin_input if origin_input else "-"
            std_origin = self.match_country_cached(origin_input) if origin_input else "-"
            
            # 3. ë§¤í•‘ ì‹¤íŒ¨ ì²´í¬
            brand_log = "ë¸Œëœë“œ ì„±ê³µ" if std_brand else f"ë¸Œëœë“œ ì‹¤íŒ¨(ì‚¬ìœ : {raw_product.raw_brand_name})"
            category_log = "ì¹´í…Œê³ ë¦¬ ì„±ê³µ" if std_cat1 else f"ì¹´í…Œê³ ë¦¬ ì‹¤íŒ¨(ì‚¬ìœ : {raw_product.gender})"
            origin_log = "ì›ì‚°ì§€ ì„±ê³µ" if std_origin else f"ì›ì‚°ì§€ ì‹¤íŒ¨(ì‚¬ìœ : {raw_product.origin or '-'})"
            
            if not std_brand or not std_cat1 or not std_origin:
                reason = f"{brand_log} / {category_log} / {origin_log}"
                log_conversion_failure(raw_product, reason)
                logger.debug(f"âŒ [ë§¤í•‘ì‹¤íŒ¨] {raw_product.external_product_id}: {reason}")
                return False
            
            # 4. ìƒí’ˆ ìƒì„±/ì—…ë°ì´íŠ¸
            product, created = Product.objects.update_or_create(
                external_product_id=raw_product.external_product_id,
                defaults={
                    'retailer': raw_product.retailer,
                    'season': raw_product.season,
                    'gender': std_cat1,
                    'category1': std_cat2,
                    'category2': std_cat3,
                    'image_url': raw_product.image_url_1,
                    'raw_brand_name': raw_product.raw_brand_name,
                    'brand_name': std_brand,
                    'product_name': raw_product.product_name,
                    'sku': raw_product.sku,
                    'price_retail': raw_product.price_retail,
                    'price_org': raw_product.price_org,
                    'discount_rate': raw_product.discount_rate,
                    'color': raw_product.color,
                    'material': raw_product.material,
                    'origin': std_origin or origin_for_save,
                    'status': 'active',
                    'created_at': raw_product.created_at,
                    'updated_at': now(),
                }
            )
            
            # 5. ì˜µì…˜ ì²˜ë¦¬ - ì¬ê³  ìˆëŠ” ê²ƒë§Œ
            raw_options = raw_product.rawoptions.filter(stock__gt=0)
            
            for opt in raw_options:
                ProductOption.objects.update_or_create(
                    product=product,
                    option_name=opt.option_name,
                    defaults={
                        'external_option_id': opt.external_option_id,
                        'stock': opt.stock,
                        'price': opt.price,
                    }
                )
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ë³€í™˜ ì˜¤ë¥˜ {raw_product.external_product_id}: {e}")
            log_conversion_failure(raw_product, f"ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")
            return False
    
    def bulk_convert_optimized(self, queryset, batch_size: int = 500) -> Tuple[int, int]:
        """ìµœì í™”ëœ ëŒ€ëŸ‰ ë³€í™˜"""
        self.stats['start_time'] = time.time()
        
        # ğŸš€ ì¿¼ë¦¬ ìµœì í™” - prefetch_relatedë¡œ N+1 ë¬¸ì œ í•´ê²°
        optimized_queryset = queryset.prefetch_related(
            Prefetch(
                'options',
                queryset=RawProductOption.objects.all(),
                to_attr='raw_options_cached'
            )
        ).select_related().iterator(chunk_size=batch_size)
        
        success_ids = []
        success_count = 0
        fail_count = 0
        
        batch_count = 0
        
        for raw_product in optimized_queryset:
            self.stats['total_processed'] += 1
            
            # ìºì‹œëœ ì˜µì…˜ ì‚¬ìš©
            raw_product.rawoptions = type('MockManager', (), {
                'aggregate': lambda self, **kwargs: {
                    'total': sum(opt.stock for opt in raw_product.raw_options_cached)
                },
                'filter': lambda self, **kwargs: [
                    opt for opt in raw_product.raw_options_cached 
                    if opt.stock > 0
                ]
            })()
            
            success = self.convert_single_product(raw_product)
            
            if success:
                success_ids.append(raw_product.id)
                success_count += 1
                self.stats['success_count'] += 1
            else:
                fail_count += 1
                self.stats['fail_count'] += 1
            
            # ğŸš€ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ìƒíƒœ ì—…ë°ì´íŠ¸ (íŠ¸ëœì­ì…˜ ìµœì í™”)
            if len(success_ids) >= batch_size:
                self._update_batch_status(success_ids)
                success_ids = []
                batch_count += 1
                
                if batch_count % 10 == 0:  # ì§„í–‰ë¥  ë¡œê·¸
                    elapsed = time.time() - self.stats['start_time']
                    processed = self.stats['total_processed']
                    rate = processed / elapsed if elapsed > 0 else 0
                    logger.info(f"ğŸ”„ ì§„í–‰: {processed}ê°œ ì²˜ë¦¬ ({rate:.1f}ê°œ/ì´ˆ)")
        
        # ë‚¨ì€ ë°°ì¹˜ ì²˜ë¦¬
        if success_ids:
            self._update_batch_status(success_ids)
        
        return success_count, fail_count
    
    def _update_batch_status(self, success_ids: List[int]):
        """ë°°ì¹˜ ë‹¨ìœ„ ìƒíƒœ ì—…ë°ì´íŠ¸"""
        if not success_ids:
            return
        
        try:
            with transaction.atomic():
                RawProduct.objects.filter(id__in=success_ids).update(
                    status='converted',
                    updated_at=now()
                )
        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def print_performance_stats(self):
        """ì„±ëŠ¥ í†µê³„ ì¶œë ¥"""
        elapsed = time.time() - self.stats['start_time'] if self.stats['start_time'] else 0
        
        print("=" * 60)
        print("ğŸ“Š ë³€í™˜ ì„±ëŠ¥ í†µê³„")
        print("=" * 60)
        print(f"ğŸ“¦ ì´ ì²˜ë¦¬: {self.stats['total_processed']:,}ê°œ")
        print(f"âœ… ì„±ê³µ: {self.stats['success_count']:,}ê°œ")
        print(f"âŒ ì‹¤íŒ¨: {self.stats['fail_count']:,}ê°œ")
        
        if self.stats['total_processed'] > 0:
            success_rate = (self.stats['success_count'] / self.stats['total_processed']) * 100
            print(f"ğŸ“ˆ ì„±ê³µë¥ : {success_rate:.1f}%")
        
        if elapsed > 0:
            rate = self.stats['total_processed'] / elapsed
            print(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ")
            print(f"ğŸš€ ì²˜ë¦¬ ì†ë„: {rate:.1f}ê°œ/ì´ˆ")
        
        # ìºì‹œ íš¨ìœ¨ì„±
        total_lookups = self.stats['cache_hits'] + self.stats['cache_misses']
        if total_lookups > 0:
            cache_hit_rate = (self.stats['cache_hits'] / total_lookups) * 100
            print(f"ğŸ¯ ìºì‹œ ì ì¤‘ë¥ : {cache_hit_rate:.1f}%")
        
        print("=" * 60)


# ğŸš€ ìµœì í™”ëœ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ìœ ì§€)

# ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤ íŒ¨í„´)
_conversion_service = None

def get_conversion_service():
    """ë³€í™˜ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤)"""
    global _conversion_service
    if _conversion_service is None:
        _conversion_service = OptimizedConversionService()
    return _conversion_service


def convert_or_update_product(raw_product):
    """ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ìœ ì§€ - ë‹¨ì¼ ìƒí’ˆ ë³€í™˜"""
    service = get_conversion_service()
    return service.convert_single_product(raw_product)


def bulk_convert_or_update_products(batch_size=500):
    """ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ìœ ì§€ - ì „ì²´ ëŒ€ëŸ‰ ë³€í™˜"""
    service = get_conversion_service()
    
    logger.info("ğŸš€ ì „ì²´ ëŒ€ëŸ‰ ë³€í™˜ ì‹œì‘...")
    
    raw_products = RawProduct.objects.filter(
        status__in=['pending', 'converted']
    )
    
    success_count, fail_count = service.bulk_convert_optimized(raw_products, batch_size)
    
    service.print_performance_stats()
    logger.info(f"âœ… ì „ì²´ ì „ì†¡ ì™„ë£Œ - ì„±ê³µ: {success_count}ê°œ / ì‹¤íŒ¨: {fail_count}ê°œ")
    
    return success_count


def bulk_convert_or_update_products_by_retailer(retailer_code, batch_size=500):
    """ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ìœ ì§€ - ê±°ë˜ì²˜ë³„ ëŒ€ëŸ‰ ë³€í™˜"""
    service = get_conversion_service()
    
    logger.info(f"ğŸš€ [{retailer_code}] ëŒ€ëŸ‰ ë³€í™˜ ì‹œì‘...")
    
    raw_products = RawProduct.objects.filter(
        retailer=retailer_code,
        status__in=['pending', 'converted']
    )
    
    success_count, fail_count = service.bulk_convert_optimized(raw_products, batch_size)
    
    service.print_performance_stats()
    logger.info(f"âœ… [{retailer_code}] ì „ì†¡ ì™„ë£Œ - ì„±ê³µ: {success_count}ê°œ / ì‹¤íŒ¨: {fail_count}ê°œ")
    
    return success_count


# ğŸš€ ì¶”ê°€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤

def analyze_conversion_bottlenecks():
    """ë³€í™˜ ë³‘ëª© ì§€ì  ë¶„ì„"""
    print("=" * 70)
    print("ğŸ” ë³€í™˜ ì„œë¹„ìŠ¤ ë³‘ëª© ì§€ì  ë¶„ì„")
    print("=" * 70)
    
    bottlenecks = {
        "ê¸°ì¡´ ë¬¸ì œì ": [
            "ë§¤ë²ˆ DBì—ì„œ Alias ë°ì´í„° ì¡°íšŒ (N+1 ë¬¸ì œ)",
            "ì¤‘ë³µ ì¿¼ë¦¬ ì‹¤í–‰ (ê°™ì€ ë¸Œëœë“œ/ì¹´í…Œê³ ë¦¬ ë°˜ë³µ ì¡°íšŒ)",
            "ê°œë³„ ì˜µì…˜ ì¬ê³  ì²´í¬ë¡œ ì¸í•œ ì¿¼ë¦¬ ì¦ê°€",
            "íŠ¸ëœì­ì…˜ ì—†ì´ ê°œë³„ ì—…ë°ì´íŠ¸",
            "prefetch_related ë¯¸ì‚¬ìš©ìœ¼ë¡œ ê´€ë ¨ ë°ì´í„° ì¤‘ë³µ ì¡°íšŒ"
        ],
        "ìµœì í™” ë°©ì•ˆ": [
            "ì‹œì‘ ì‹œ ëª¨ë“  Alias ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ ìºì‹œë¡œ ë¡œë“œ",
            "O(1) ì‹œê°„ë³µì¡ë„ì˜ ë”•ì…”ë„ˆë¦¬ ê¸°ë°˜ ë§¤ì¹­",
            "ì§‘ê³„ ì¿¼ë¦¬ë¡œ ì¬ê³  ì²´í¬ ìµœì í™”",
            "ë°°ì¹˜ ë‹¨ìœ„ íŠ¸ëœì­ì…˜ ì²˜ë¦¬",
            "prefetch_relatedë¡œ ê´€ë ¨ ì˜µì…˜ ë°ì´í„° ë¯¸ë¦¬ ë¡œë“œ"
        ],
        "ì„±ëŠ¥ ê°œì„ ": [
            "ë§¤í•‘ ì†ë„: O(n) â†’ O(1) (í•´ì‹œ í…Œì´ë¸”)",
            "ì¿¼ë¦¬ ìˆ˜: 1000ë°° ê°ì†Œ (ìºì‹œ ì‚¬ìš©)",
            "ë©”ëª¨ë¦¬ ì‚¬ìš©: ì•½ê°„ ì¦ê°€í•˜ì§€ë§Œ ì†ë„ ëŒ€í­ í–¥ìƒ",
            "ì „ì²´ ì²˜ë¦¬ ì‹œê°„: 80-90% ë‹¨ì¶• ì˜ˆìƒ"
        ]
    }
    
    for category, items in bottlenecks.items():
        print(f"\nğŸ”§ {category}:")
        for item in items:
            print(f"   â€¢ {item}")
    
    print("\nğŸ“Š ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ:")
    print("   â€¢ ì²˜ë¦¬ ì†ë„: 5-10ë°° í–¥ìƒ")
    print("   â€¢ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ì•½ê°„ ì¦ê°€ (ìºì‹œ)")
    print("   â€¢ DB ì¿¼ë¦¬ ìˆ˜: 95% ì´ìƒ ê°ì†Œ")
    print("   â€¢ ì „ì²´ ì‹œê°„: 80-90% ë‹¨ì¶•")
    
    print("=" * 70)


def reset_conversion_cache():
    """ìºì‹œ ì´ˆê¸°í™” (ë°ì´í„° ë³€ê²½ ì‹œ ì‚¬ìš©)"""
    global _conversion_service
    _conversion_service = None
    logger.info("ğŸ”„ ë³€í™˜ ì„œë¹„ìŠ¤ ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")


if __name__ == "__main__":
    # ë³‘ëª© ì§€ì  ë¶„ì„
    analyze_conversion_bottlenecks()